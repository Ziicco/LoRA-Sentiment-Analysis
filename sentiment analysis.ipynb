{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea22e25c-9895-4922-9d79-2d1f94e7caaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "import logging\n",
    "from peft import get_peft_model, LoraConfig, TaskType  # LoRA integration\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e5766-a7f9-41fd-b6bc-23d5f9580daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for better debugging and production readiness\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a39363-d710-41a9-947c-3bed6f65d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5383b6-89b6-4aae-9e2f-78010ad7be52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained sentiment analysis model from HuggingFace\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf66758-6e55-420e-9698-e27ec0440850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d1987-4cb7-45a7-9b21-c91323930472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for saving checkpoints\n",
    "CHECKPOINT_DIR = \"model_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d3fe8-5a4c-4d3d-b6be-89ca38dc0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hyperparameter tuning using Optuna\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 32, step=8)\n",
    "\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    train_full = load_dataset(\"imdb\", split=\"train\")\n",
    "    train_shuffled = train_full.shuffle(seed=42)\n",
    "    train_dataset = train_shuffled.select(range(2000))\n",
    "    train_dataset = preprocess_data(tokenizer, train_dataset)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    train_model(model, train_loader, optimizer, lr_scheduler, device)\n",
    "\n",
    "    test_full = load_dataset(\"imdb\", split=\"test\")\n",
    "    test_shuffled = test_full.shuffle(seed=42)\n",
    "    test_dataset = test_shuffled.select(range(500))\n",
    "    test_dataset = preprocess_data(tokenizer, test_dataset)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    predictions, labels = evaluate_model(model, test_loader, device)\n",
    "    return f1_score(labels, predictions, average='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dab23-c6ce-44ea-9758-b41e3c1acdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Define the target modules for DistilBERT\n",
    "    target_modules = [\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]  # Target Linear layers in attention\n",
    "    # target_modules = [\"pre_classifier\", \"classifier\"]\n",
    "    \n",
    "    # Integrate LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,                   # Task type (Sequence Classification)\n",
    "        inference_mode=False,                         # Enable training mode\n",
    "        r=16,                                         # LoRA rank\n",
    "        lora_alpha=32,                                # Scaling factor\n",
    "        lora_dropout=0.05,                            # Dropout rate\n",
    "        target_modules=target_modules                 # Specify target modules\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    logging.info(\"LoRA model initialized.\")\n",
    "\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af58987-e90e-4a6e-90c4-195c3ac97616",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_data(tokenizer, dataset):\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b5a68-a005-4610-ad31-525e2a07b988",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_model(model, train_loader, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        logging.info(f\"Starting epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # Log training loss to TensorBoard\n",
    "                writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_loader) + i)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during training: {e}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"model_epoch_{epoch + 1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        logging.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        logging.info(f\"Epoch {epoch + 1} completed. Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06edf8fd-6ee5-420e-ba2f-5f01c97b6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels.extend(batch['label'].tolist())\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during evaluation: {e}\")\n",
    "\n",
    "    return predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d921c3-bb63-4dfa-bed9-21fe55f40f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify text\n",
    "def classify_text(model, tokenizer, text, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "            probabilities = torch.softmax(logits, dim=-1).squeeze().tolist()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during text classification: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    return predicted_class, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782959e-7ff1-417e-9ce5-c10b7816cffa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    logging.info(\"Best hyperparameters:\")\n",
    "    logging.info(study.best_params)\n",
    "\n",
    "    # Load best hyperparameters\n",
    "    best_learning_rate = study.best_params[\"learning_rate\"]\n",
    "    best_batch_size = study.best_params[\"batch_size\"]\n",
    "\n",
    "    train_full = load_dataset(\"imdb\", split=\"train\")\n",
    "    train_shuffled = train_full.shuffle(seed=42)\n",
    "    train_dataset = train_shuffled.select(range(2000))\n",
    "    train_dataset = preprocess_data(tokenizer, train_dataset)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    test_full = load_dataset(\"imdb\", split=\"test\")\n",
    "    test_shuffled = test_full.shuffle(seed=42)\n",
    "    test_dataset = test_shuffled.select(range(500))\n",
    "    test_dataset = preprocess_data(tokenizer, test_dataset)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=best_learning_rate)\n",
    "    num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    logging.info(\"Starting fine-tuning the model.\")\n",
    "    train_model(model, train_loader, optimizer, lr_scheduler, device)\n",
    "\n",
    "    logging.info(\"Evaluating the model.\")\n",
    "    predictions, labels = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='binary')\n",
    "    recall = recall_score(labels, predictions, average='binary')\n",
    "    f1 = f1_score(labels, predictions, average='binary')\n",
    "\n",
    "    logging.info(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    logging.info(f\"Precision: {precision * 100:.2f}%\")\n",
    "    logging.info(f\"Recall: {recall * 100:.2f}%\")\n",
    "\n",
    "    logging.info(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "    logging.info(\"\\nConfusion Matrix:\")\n",
    "    logging.info(confusion_matrix(labels, predictions))\n",
    "\n",
    "    while True:\n",
    "        text = input(\"\\nEnter text for sentiment analysis (or type 'exit' to quit): \")\n",
    "        if text.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        predicted_class, probabilities = classify_text(model, tokenizer, text, device)\n",
    "        sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "\n",
    "        print(f\"Sentiment: {sentiment}\")\n",
    "        print(f\"Probabilities: {probabilities}\")\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34bf1f-4a5e-46f2-a7a1-acc940d39f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df60945-3528-46ab-afc0-0e3a0cefcbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:projects] *",
   "language": "python",
   "name": "conda-env-projects-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
